\section{Literature Assessment}
\label{sec:literature}

\subsection{Background and Foundations}
\label{sec:lit-background}

The rapid advancement of large language models (LLMs) has given rise to \emph{agentic AI systems}---autonomous or semi-autonomous entities that leverage LLMs to perceive their environment, reason about tasks, and take actions toward defined goals~\cite{villaescusa2025denario, trinkenreich2025train}. A core capability enabling this progression is \emph{tool use}: agents extend their reasoning by invoking external tools such as code interpreters, search engines, and databases, transforming LLMs from passive text generators into active participants in computational workflows~\cite{baltes2025guidelines}.

Two complementary knowledge paradigms support these systems. \emph{Knowledge graphs} (KGs) provide explicit, structured representations of domain knowledge as networks of entities and relations~\cite{suryawanshi2025kgrag}, while \emph{retrieval-augmented generation} (RAG) grounds model outputs in dynamically retrieved documents to reduce hallucination. Both externalize knowledge into inspectable, queryable structures. Yet not all knowledge available to an agent can be externalized so neatly. Sharkey et al.~\cite{sharkey2025mechanistic} demonstrate that significant knowledge resides \emph{implicitly} within LLM parameters: the \emph{linear representation hypothesis} posits that concepts are encoded as directions in embedding spaces, while the \emph{superposition hypothesis} explains how networks represent more features than they have dimensions through sparse encoding. An agent's reasoning thus draws on both explicit retrieved knowledge \emph{and} implicit parametric knowledge---raising questions about where agent ``knowledge'' resides and how reliably it can be traced to its sources.

These capabilities have outpaced the methodological frameworks for evaluating them. Ralph et al.~\cite{ralph2021empirical} established community-driven empirical standards for SE research---methodology-specific checklists codifying essential, desirable, and extraordinary attributes---yet these standards are designed for and governed by the SE community through volunteer consensus and predate the LLM era. Baltes et al.~\cite{baltes2025guidelines} extend this framework with LLM-specific guidelines covering model versioning, prompt reporting, and agent traceability, and provide a taxonomy of seven LLM study types. However, these guidelines remain untested expert recommendations without empirical evidence that adherence improves study quality. Together, these works highlight a recurring tension: \emph{the capabilities of agentic AI systems advance faster than the standards needed to evaluate them}.

\subsection{Agentic Architectures and Frameworks}
\label{sec:lit-architectures}

The surveyed papers reveal a spectrum of architectural approaches, ranging from static pipelines~\cite{suryawanshi2025kgrag} to orchestrated multi-agent frameworks~\cite{villaescusa2025denario}, with significant implications for reliability and scope.

At one end, Suryawanshi et al.~\cite{suryawanshi2025kgrag} present a multi-stage pipeline for cross-document information extraction---from document chunking through knowledge graph construction, community detection, and hybrid retrieval. This architecture operates as a \emph{deterministic pipeline} without agent autonomy; each stage executes sequentially with no capacity for self-correction. Its strength lies in this predictability: outputs are inspectable and reproducible, though limited to the narrow task of structured knowledge extraction.

At the other end, Villaescusa-Navarro et al.~\cite{villaescusa2025denario} introduce Denario, a modular multi-agent system with six specialized modules orchestrated through a \emph{Planning \& Control} strategy adapted from robotics. Although its authors frame Denario as a ``scientific research assistant'' whose ``goal is not to automate science''~\cite{villaescusa2025denario}, the architecture \emph{can} execute end-to-end research without human intervention. Its support for multiple LLM backends and orchestration frameworks demonstrates that the agentic layer can be decoupled from the underlying model, enabling partial or complete automation.

Trinkenreich et al.~\cite{trinkenreich2025train} take a different approach, using McLuhan's Tetrad of Media Laws to analyze how LLMs reshape the research pipeline. Their framework, however, remains entirely theoretical---no case studies or empirical validation accompany the analysis. They advocate for \emph{human-in-the-loop} architectures, warning that full autonomy risks ``creativity echo chambers'' where outputs ``reflect common patterns rather than genuinely novel insights.'' Denario's authors share this concern, recommending the system ``functions optimally as an assistant rather than an autonomous researcher''~\cite{villaescusa2025denario}---yet the architecture imposes no such constraint, revealing a tension between \emph{capability} and \emph{recommended use} that carries epistemic consequences for research originality.

A critical pattern emerges across these paradigms: \emph{evaluation rigor does not match architectural ambition}. Denario relies on single-expert numerical scoring without inter-rater reliability~\cite{villaescusa2025denario}; the KG-RAG system offers only qualitative evaluation without quantitative baselines~\cite{suryawanshi2025kgrag}; and Trinkenreich et al.~\cite{trinkenreich2025train} provide a speculative framework without validation. By the criteria Ralph et al.~\cite{ralph2021empirical} and Baltes et al.~\cite{baltes2025guidelines} prescribe, these evaluations would benefit from operationalized constructs, appropriate baselines, and validity discussions. Sharkey et al.~\cite{sharkey2025mechanistic} identify a related concern: without mechanistic understanding of how agents arrive at outputs, it is difficult to distinguish genuine capability from superficial pattern matching---a risk they term ``interpretability illusions.''

\subsection{Knowledge Representation and Reasoning}
\label{sec:lit-knowledge}

How agentic research systems represent, store, and reason over knowledge is a fundamental design decision. The surveyed papers illuminate a spectrum---from fully explicit to largely implicit representation---and suggest that neither extreme alone supports reliable autonomous research.

Toward the explicit end, Suryawanshi et al.~\cite{suryawanshi2025kgrag} construct a domain-specific knowledge graph from research papers, using entity description augmentation to bridge structured graph representations with vector search. Ralph et al.~\cite{ralph2021empirical} similarly represent methodological knowledge as modular checklists---effectively a rule-based knowledge graph of community standards. Baltes et al.~\cite{baltes2025guidelines} extend this paradigm to LLM research practices through formalized guidelines with RFC~2119 requirement levels. These approaches prioritize transparency and auditability: the knowledge is inspectable, versioned, and subject to community review.

Toward the implicit end, Sharkey et al.~\cite{sharkey2025mechanistic} document how neural networks represent concepts as directions in embedding spaces and encode more features than they have dimensions through sparse superposition. As a survey and position paper, their work contributes no novel empirical methods and remains heavily transformer-centric despite calling for broader architectural coverage. Current decomposition methods---sparse autoencoders, transcoders, and concept-based probes---remain limited; a sparse dictionary with 16 million latents inserted into GPT-4 produced language modeling loss equivalent to a model with only 10\% of its pretraining compute, quantifying the gap between implicit knowledge and our ability to extract it. Denario~\cite{villaescusa2025denario} operates at this implicit end, drawing on parametric knowledge to generate research across multiple scientific disciplines. However, its knowledge is \emph{ephemeral}: each session starts from the LLM's training-time knowledge plus injected context, with no persistent store across runs.

The consequences for research reliability are direct. Explicit knowledge stores like Suryawanshi et al.'s~\cite{suryawanshi2025kgrag} KG are transparent and auditable but limited in scale and incapable of autonomous reasoning beyond retrieval. Implicit approaches enable cross-disciplinary synthesis but produce outputs that cannot be traced to specific sources---a critical limitation when systems hallucinate, as documented in Denario's cyclic peptide failure where agents ``hallucinated an entire paper without implementing the necessary numerical solver''~\cite{villaescusa2025denario}. Trinkenreich et al.~\cite{trinkenreich2025train} identify the downstream risk: when systems ``generate content based on existing knowledge,'' they risk ``homogenization of research rather than groundbreaking discoveries.'' Sharkey et al.'s concept of ``microscope AI''~\cite{sharkey2025mechanistic}---using interpretability to extract novel knowledge from trained models---suggests a promising direction, but this capability remains aspirational for systems at the complexity of multi-agent research frameworks.

\subsection{Autonomous Research and Discovery}
\label{sec:lit-research}

The surveyed work spans a spectrum of research automation, from LLMs as productivity tools~\cite{trinkenreich2025train} to systems capable of executing entire research cycles~\cite{villaescusa2025denario}, with targeted sub-task automation in between~\cite{suryawanshi2025kgrag}.

Trinkenreich et al.~\cite{trinkenreich2025train} position LLMs as \emph{research tools} that enhance human capabilities, emphasizing that ``the true transformative potential lies in enabling researchers to redirect their time toward deeper intellectual engagement.'' At the other end, Denario~\cite{villaescusa2025denario} \emph{can} execute the full research cycle, producing papers ``at the level of what a good undergraduate or early graduate student could achieve'' in approximately 30 minutes---though its authors emphasize the system should serve as an assistant that accelerates discovery rather than replaces researchers. Between these positions, Suryawanshi et al.~\cite{suryawanshi2025kgrag} automate a specific sub-task---cross-document information extraction---demonstrating that even narrow automation can yield useful research infrastructure.

Evaluating autonomous research outputs presents significant methodological challenges. Denario's expert scoring shows most AI-generated papers score above average, yet the evaluation proceeds without inter-rater reliability, baselines, or operationalized quality definitions---falling short of the criteria both Ralph et al.~\cite{ralph2021empirical} and Baltes et al.~\cite{baltes2025guidelines} prescribe. Systematic failure modes compound the problem: Denario exhibits confirmation bias, tending to ``overstate the findings'' and avoid negative results~\cite{villaescusa2025denario}, and produces citations that domain experts can ``easily spot'' as AI-generated. Trinkenreich et al.~\cite{trinkenreich2025train} warn of ``systemic bias where LLMs reinforce each other's errors, creating a false perception of reliability.'' Sharkey et al.~\cite{sharkey2025mechanistic} argue that mechanistic interpretability could provide diagnostic tools for understanding \emph{why} an agent reaches a conclusion, though such tools do not yet operate at the scale required for multi-agent research systems.

\subsection{Synthesis and Gap Analysis}
\label{sec:lit-synthesis}

The surveyed literature exposes three distinct categories of gap, which we address in turn.

\textbf{Architectural capability gaps.} No existing system tightly integrates persistent, structured knowledge representation with autonomous multi-agent research workflows. Denario~\cite{villaescusa2025denario} can orchestrate end-to-end research workflows but lacks a persistent, queryable knowledge store across sessions; KG-RAG~\cite{suryawanshi2025kgrag} maintains structured knowledge but has no agentic reasoning layer. Neither system supports cumulative learning: each research session begins from scratch, unable to build on prior investigations or maintain evolving research context.

\textbf{Evaluation gaps.} Ralph et al.'s~\cite{ralph2021empirical} empirical standards did not anticipate LLM-era challenges, and Baltes et al.'s~\cite{baltes2025guidelines} extensions do not yet address autonomous research agents that generate entire studies. Denario~\cite{villaescusa2025denario} exemplifies this tension: it demonstrates exactly the kind of system these standards need to cover, yet its own evaluation---single-expert scoring without inter-rater reliability or baselines---does not meet the criteria either framework prescribes. No validated evaluation framework exists for AI-generated research outputs.

\textbf{Provenance and faithfulness gaps.} Explicit knowledge approaches (KG-RAG~\cite{suryawanshi2025kgrag}, empirical standards~\cite{ralph2021empirical}) offer auditability at the cost of limited scope. Implicit approaches (Denario~\cite{villaescusa2025denario}) achieve cross-disciplinary synthesis but shift traceability into opaque parametric representations where provenance cannot yet be systematically recovered. Sharkey et al.~\cite{sharkey2025mechanistic} identify mechanistic interpretability as a source of diagnostic insight into implicit representations, but current methods provide post-hoc analysis rather than actionable provenance guarantees, and do not yet operate at the scale of multi-agent systems. Trinkenreich et al.~\cite{trinkenreich2025train} suggest human-in-the-loop oversight as a practical mitigation, though this partially negates the efficiency gains that motivate automation. The result is that when an agentic system generates a research claim, there is currently no generalizable, systematically enforced mechanism to trace that claim back to specific evidence---whether from retrieved documents, knowledge graph entities, or parametric model knowledge.

These three gap categories are not independent. Architectural limitations make provenance tracking infeasible; the absence of provenance makes rigorous evaluation impossible; and without evaluation criteria, architectural design lacks a target to optimize against. Breaking this cycle requires addressing provenance directly. Concretely, trustworthy autonomous research demands two capabilities that no surveyed system provides: \emph{span-level evidence alignment}, in which every generated claim is linked to specific source passages or knowledge graph substructures that support it, and \emph{canonicalization auditing}, in which entity references are resolved to canonical knowledge graph nodes so that reviewers and automated validators can verify whether cited evidence actually supports the claims derived from it. Without these mechanisms, agentic research outputs remain opaque regardless of how sophisticated the underlying architecture becomes. Addressing this requires integrating explicit knowledge graphs into agentic research workflows so that every generated claim is grounded in traceable, verifiable evidence---an approach we develop in the following section. We note, however, that even with span-level provenance enforcement, agentic systems remain bounded by the distribution of existing literature and training data, raising a deeper question about whether such automation enables genuinely novel inquiry or primarily reinforces existing research trajectories.
