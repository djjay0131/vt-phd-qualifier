\section{Literature Assessment}
\label{sec:literature}

\subsection{Background and Foundations}
\label{sec:lit-background}

The rapid advancement of large language models (LLMs) has given rise to \emph{agentic AI systems}---autonomous or semi-autonomous computational entities that leverage LLMs to perceive their environment, reason about complex tasks, and take actions toward defined goals~\cite{villaescusa2025denario, trinkenreich2025train}. These systems range from simple single-model pipelines that execute predefined workflows to sophisticated multi-agent architectures where specialized agents collaborate through structured communication protocols. A core capability enabling this progression is \emph{tool use}: agents extend their reasoning by invoking external tools such as code interpreters, search engines, and database queries, transforming LLMs from passive text generators into active participants in computational workflows~\cite{baltes2025guidelines}. Baltes et al. formalize this distinction, noting that agent-based systems require documenting three distinct components: internal LLM deliberation, external tool invocations, and human or multi-agent interactions.

Two complementary knowledge infrastructure paradigms support agentic research systems. \emph{Knowledge graphs} (KGs) provide explicit, structured representations of domain knowledge as networks of entities and relations, enabling systematic retrieval and reasoning over factual information~\cite{suryawanshi2025kgrag}. \emph{Retrieval-augmented generation} (RAG) addresses the limitations of static LLM training knowledge by grounding model outputs in dynamically retrieved external documents, reducing hallucination and enabling domain specialization. These approaches externalize knowledge into inspectable, queryable structures. However, Sharkey et al.~\cite{sharkey2025mechanistic} reveal that significant knowledge also resides \emph{implicitly} within LLM parameters themselves---the \emph{linear representation hypothesis} posits that high-level concepts are encoded as directions in neural network embedding spaces, while the \emph{superposition hypothesis} explains how networks represent more features than they have dimensions through sparse encoding. These findings raise fundamental questions about where agent ``knowledge'' truly lives and how reliably it can be accessed.

These technological capabilities have outpaced the methodological frameworks for evaluating them. Ralph et al.~\cite{ralph2021empirical} established community-driven empirical standards for software engineering research---methodology-specific checklists that codify evaluation criteria into essential, desirable, and extraordinary attributes---yet these standards predate the LLM era and do not address AI-assisted research workflows. Recognizing this gap, Baltes et al.~\cite{baltes2025guidelines} extend the standards framework with LLM-specific guidelines covering model versioning, prompt reporting, and agent behavior traceability. Their taxonomy of seven LLM study types---from LLMs as annotators and judges to LLMs as subjects of study---provides the conceptual vocabulary for classifying the diverse ways LLMs now participate in research. Together, these works highlight a recurring tension: \emph{the capabilities of agentic AI systems consistently advance faster than the standards needed to rigorously evaluate them}.

\subsection{Agentic Architectures and Frameworks}
\label{sec:lit-architectures}

The surveyed papers reveal a spectrum of architectural approaches for structuring AI-assisted research systems, ranging from static pipelines to fully orchestrated multi-agent frameworks. The design choices at each level---degree of autonomy, coordination mechanism, and modularity---carry significant implications for the reliability and scope of the research these systems can conduct.

At one end of this spectrum, Suryawanshi et al.~\cite{suryawanshi2025kgrag} present a five-stage pipeline for cross-document information extraction: document chunking, knowledge graph construction (entity recognition and relation extraction with domain-specific types), community detection via the Louvain algorithm, hierarchical summarization, and hybrid vector-graph retrieval. While effective for its narrow task---extracting structured knowledge from a corpus of research papers---this architecture operates as a \emph{deterministic pipeline} without agent autonomy; each stage executes sequentially with no capacity for self-correction or adaptive replanning. The pipeline's strength lies precisely in this predictability: outputs at each stage are inspectable and reproducible.

By contrast, Villaescusa-Navarro et al.~\cite{villaescusa2025denario} introduce Denario, a modular multi-agent system comprising six specialized modules (Idea, Literature, Methods, Analysis, Paper, Review) orchestrated through a \emph{Planning \& Control} strategy adapted from robotics. During planning, a planner agent and plan reviewer agent collaboratively decompose research tasks into 3--8 subtasks through iterative refinement; during control, a monitor agent tracks execution status, records whether steps complete or fail, and coordinates agent handoffs with a hard limit of 500 messages to prevent infinite loops. This architecture enables end-to-end scientific research without human intervention, from hypothesis generation through paper drafting. The system supports multiple LLM backends (GPT-5, Gemini 2.5 Pro, Claude Opus) across two orchestration frameworks---AG2 and LangGraph---demonstrating that the agentic layer can be decoupled from the underlying model. This modularity allows researchers to use individual modules (e.g., only idea generation) or the full pipeline, supporting partial as well as complete automation.

Trinkenreich et al.~\cite{trinkenreich2025train} do not propose a specific architecture but instead use McLuhan's Tetrad of Media Laws to systematically analyze how LLMs reshape every stage of the six-stage research pipeline---from research question formulation through writing and dissemination. Their analysis implicitly advocates for \emph{human-in-the-loop} architectures where LLMs augment rather than replace researchers, warning that full autonomy risks creating ``creativity echo chambers'' where generated research questions ``reflect common patterns rather than genuinely novel insights.'' This position stands in productive tension with Denario's fully autonomous approach, suggesting that architectural decisions about autonomy level carry epistemic consequences for the originality and reliability of research outputs.

A critical gap emerges across these architectural paradigms: \emph{evaluation rigor does not match architectural ambition}. Denario evaluates its outputs through single-expert numerical scoring (0--10) without inter-rater reliability assessment~\cite{villaescusa2025denario}; the KG-RAG system provides only qualitative evaluation with no quantitative baselines~\cite{suryawanshi2025kgrag}; and Trinkenreich et al.~\cite{trinkenreich2025train} offer a speculative framework without empirical validation. By the standards that Ralph et al.~\cite{ralph2021empirical} established for SE research and that Baltes et al.~\cite{baltes2025guidelines} extended to LLM-involving studies, these evaluations lack essential attributes such as operationalized constructs, appropriate baselines, and validity discussions. Sharkey et al.~\cite{sharkey2025mechanistic} identify a related concern at a deeper level: without mechanistic understanding of how LLM agents arrive at their outputs, it is difficult to distinguish genuine capability from superficial pattern matching---a problem they term the risk of ``interpretability illusions'' where plausible-seeming explanations mask fundamentally different underlying mechanisms.

\subsection{Knowledge Representation and Reasoning}
\label{sec:lit-knowledge}

A fundamental architectural decision facing agentic research systems is how to represent, store, and reason over the knowledge they require. The surveyed papers illuminate two contrasting paradigms---explicit and implicit knowledge representation---and reveal that neither alone is sufficient for reliable autonomous research.

\textbf{Explicit knowledge representation} externalizes domain knowledge into structured, queryable artifacts. Suryawanshi et al.~\cite{suryawanshi2025kgrag} construct a domain-specific knowledge graph from research papers, extracting 2,601 entities and 3,052 relations organized into 311 communities via Louvain clustering. Their novel \emph{entity description augmentation} technique bridges structured graph representations with vector search by enriching entity nodes with natural language descriptions, enabling hybrid retrieval that leverages both graph topology and semantic similarity. Similarly, Ralph et al.~\cite{ralph2021empirical} represent methodological knowledge as modular, structured checklists---each methodology mapped to a set of essential, desirable, and extraordinary attributes with decision-tree logic for evaluating compliance---creating what is effectively a \emph{rule-based knowledge graph} of community research standards. Both approaches prioritize \emph{transparency and auditability}: the knowledge is inspectable, versioned through GitHub-based governance, and subject to community review. Baltes et al.~\cite{baltes2025guidelines} extend this explicit paradigm to LLM research practices, encoding expert consensus about study design into formalized guidelines with RFC~2119 requirement levels (must/should/may).

\textbf{Implicit knowledge representation} relies on knowledge encoded within LLM parameters. Sharkey et al.~\cite{sharkey2025mechanistic} provide the most comprehensive analysis of this paradigm, documenting how neural networks represent concepts as directions in embedding spaces (the linear representation hypothesis) and encode more features than they have dimensions through sparse superposition. Current methods for accessing this knowledge---sparse autoencoders, transcoders, and concept-based probes---remain limited; when a sparse dictionary with 16 million latents was inserted into GPT-4, the resulting language modeling loss was equivalent to a model with only 10\% of its pretraining compute, quantifying the significant information loss in current decomposition approaches. Despite these limitations, implicit knowledge is powerful: it enables the cross-domain reasoning that makes systems like Denario~\cite{villaescusa2025denario} capable of generating research across 13 scientific disciplines---including a paper combining quantum physics methods with cosmological data that required expertise spanning three distinct fields. However, Denario's knowledge is \emph{ephemeral}: each research session starts from the LLM's training-time knowledge plus injected context, with no persistent knowledge store across runs.

The tension between these paradigms has direct consequences for research reliability. Suryawanshi et al.'s~\cite{suryawanshi2025kgrag} explicit KG is transparent and auditable but limited in scale (10 papers) and incapable of autonomous reasoning beyond retrieval. Denario's~\cite{villaescusa2025denario} implicit approach enables rich cross-disciplinary synthesis but produces outputs that cannot be traced to specific knowledge sources---a critical limitation when the system exhibits confirmation bias or hallucinates results, as documented in its cyclic peptide failure case where agents ``hallucinated an entire paper without implementing the necessary numerical solver.'' Trinkenreich et al.~\cite{trinkenreich2025train} identify the downstream consequence: when LLMs ``generate content based on existing knowledge,'' systems risk ``homogenization of research rather than groundbreaking discoveries.'' Baltes et al.~\cite{baltes2025guidelines} offer a practical but incomplete response through their guideline on reporting tool architectures, requiring researchers to document multi-component knowledge flows---yet this addresses transparency of \emph{use} without solving the underlying opacity of \emph{representation}. Sharkey et al.'s concept of ``microscope AI''---using interpretability to extract novel scientific knowledge from trained models---suggests a potential bridge, but this capability remains aspirational for systems operating at the complexity of multi-agent research frameworks.

\subsection{Autonomous Research and Discovery}
\label{sec:lit-research}

The papers surveyed span a clear spectrum of research automation, from LLMs as productivity tools to LLMs as autonomous research agents. Understanding where current systems fall on this spectrum---and the trade-offs at each level---is essential for identifying what remains to be built.

At the tool end, Trinkenreich et al.~\cite{trinkenreich2025train} position LLMs primarily as \emph{research tools} that enhance human capabilities across the research pipeline: automating systematic literature reviews, assisting with qualitative coding, suggesting statistical techniques, and supporting interdisciplinary theory retrieval. Their framework emphasizes that ``the true transformative potential lies in enabling researchers to redirect their time toward deeper intellectual engagement''---a vision of \emph{augmented} rather than automated research. They note that LLMs can retrieve previously ``obsolesced'' research practices, including the informal, collaborative ideation culture of ``coffee house research'' that has diminished in modern academia. At the opposite end, Villaescusa-Navarro et al.~\cite{villaescusa2025denario} demonstrate that Denario can execute the full research cycle autonomously, producing papers ``at the level of what a good undergraduate or early graduate student could achieve after weeks or a few months of work'' in approximately 30 minutes at a cost of \$4. Denario has been tested across 13 scientific disciplines---from astrophysics to neuroscience to chemistry---with one AI-generated paper accepted at the Agents4Science 2025 conference. Between these extremes, Suryawanshi et al.~\cite{suryawanshi2025kgrag} automate a specific research sub-task---cross-document information extraction---without broader research autonomy, demonstrating that even narrow automation can produce useful research infrastructure.

The evaluation of autonomous research outputs reveals significant methodological challenges that cut across all levels of the autonomy spectrum. Denario's expert scoring shows that most AI-generated papers score above average (5/10), with some reaching 8--9, yet the evaluation itself fails essential criteria from both Ralph et al.~\cite{ralph2021empirical} and Baltes et al.~\cite{baltes2025guidelines}: no inter-rater reliability, no baselines, and subjective quality definitions. More concerning are systematic failure modes that emerge at higher autonomy levels. Denario exhibits confirmation bias, tending to ``overstate the findings'' and avoiding negative results---a pattern its authors attribute to the positive-results bias in training data~\cite{villaescusa2025denario}. It also struggles with citations, producing references that domain experts can ``easily spot'' as AI-generated. Trinkenreich et al.~\cite{trinkenreich2025train} warn of a related danger: ``systemic bias where LLMs reinforce each other's errors, creating a false perception of reliability.'' Sharkey et al.~\cite{sharkey2025mechanistic} argue that mechanistic interpretability could provide the missing diagnostic tools---understanding \emph{why} an agent reaches a conclusion would enable verification beyond surface-level expert review, potentially achieving what they term ``enumerative safety'' for research agents.

\subsection{Synthesis and Gap Analysis}
\label{sec:lit-synthesis}

Three cross-cutting patterns emerge from the surveyed literature, each pointing toward specific gaps that current work leaves unaddressed.

\textbf{Standards consistently lag capabilities.} Ralph et al.'s~\cite{ralph2021empirical} empirical standards (2021) established a rigorous framework for SE research but did not anticipate LLM-era methodological challenges. Baltes et al.~\cite{baltes2025guidelines} partially close this gap with LLM-specific guidelines (2025), yet neither framework addresses the evaluation of \emph{autonomous research agents} that generate entire studies. Denario~\cite{villaescusa2025denario} demonstrates exactly the kind of system these standards need to cover, but its own evaluation methodology does not meet the standards either framework prescribes---a paradox where the system that most needs rigorous evaluation is the one least rigorously evaluated. This creates an urgent need for evaluation criteria specifically designed for AI-generated research---criteria that address output quality, process transparency, knowledge provenance, and failure mode detection.

\textbf{Transparency and capability trade off against each other.} Explicit knowledge approaches (KG-RAG~\cite{suryawanshi2025kgrag}, empirical standards~\cite{ralph2021empirical}) offer transparency and auditability at the cost of limited scope and no autonomy. Implicit approaches (Denario~\cite{villaescusa2025denario}) achieve cross-disciplinary synthesis and genuine autonomous research capability but sacrifice interpretability, producing outputs that cannot be traced to specific knowledge sources. Sharkey et al.~\cite{sharkey2025mechanistic} identify the theoretical bridge---mechanistic interpretability could make implicit LLM knowledge explicit---but the field has not yet produced tools that work at the scale and complexity of agentic research systems. Trinkenreich et al.~\cite{trinkenreich2025train} suggest that human-in-the-loop oversight may be the most practical near-term mitigation, yet this partially negates the efficiency gains that motivate autonomous research in the first place.

These patterns converge on three concrete gaps. \emph{First}, no existing system integrates structured knowledge representation with autonomous multi-agent research: Denario lacks persistent, queryable knowledge across sessions; KG-RAG lacks research agency. A hybrid architecture combining explicit knowledge graphs with agentic research workflows---where agents build, query, and evolve a knowledge graph as they conduct research---could address both limitations simultaneously. \emph{Second}, no validated evaluation framework exists for AI-generated research outputs---the gap between what Denario \emph{produces} and what Ralph et al. and Baltes et al. \emph{require} remains unbridged, leaving the field without principled criteria for assessing when AI-generated research is trustworthy. \emph{Third}, current agentic systems operate without cumulative learning: each research session begins from scratch, unable to build on prior investigations or maintain evolving research context. Addressing these gaps requires integrating persistent knowledge structures that evolve across research sessions, combining the transparency of explicit knowledge graphs with the reasoning power of LLM-based agents---an architecture we propose in the following section.
