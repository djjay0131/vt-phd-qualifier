\section{Introduction}
\label{sec:introduction}

Large language models have moved beyond isolated text generation into a new operational role: as components of \emph{semi-autonomous agentic systems} that perceive research contexts, invoke external tools, and coordinate multi-step workflows with limited human oversight~\cite{villaescusa2025denario, trinkenreich2025train}. Where earlier applications treated language models as question-answering interfaces, emerging architectures orchestrate multiple specialized agents—responsible for tasks such as literature retrieval, data analysis, and manuscript drafting—within pipelines capable of executing substantial portions of the research cycle~\cite{villaescusa2025denario}. This shift from isolated model usage to coordinated, tool-using agent systems represents a qualitative change in how AI participates in scientific work.

As these systems transition from experimental prototypes to components of real research workflows, the need for supporting infrastructure becomes increasingly evident. Consider a concrete scenario: a doctoral student uses an agentic system to identify a promising open problem from a corpus of recent publications. Weeks later, new papers appear that reframe the problem or resolve it entirely, but the system retains no persistent record of the original problem context, the evidence chains that motivated it, or how it relates to the student's evolving research trajectory. Without a canonical, queryable representation of research problems across sessions, the student must manually re-verify novelty, re-trace evidence, and reconcile conflicting formulations---precisely the labor that agentic systems are meant to reduce.

This scenario illustrates three infrastructure requirements that current systems do not adequately address. Without \emph{persistence}, research knowledge accumulated in one session decays before the next, forcing redundant re-extraction. Without \emph{traceability}, generated claims cannot be verified against their source evidence, leaving reviewers unable to distinguish well-grounded outputs from hallucination. Without \emph{evaluability}---operationalized metrics for measuring provenance completeness, extraction reliability, and progression quality---there is no systematic way to assess whether an agentic system's outputs are improving or degrading over time. Current approaches do not tightly integrate persistent knowledge representations with agentic reasoning, do not provide validated evaluation frameworks tailored to agent-generated outputs, and do not systematically enforce traceability between generated claims and their supporting evidence~\cite{suryawanshi2025kgrag, ralph2021empirical, baltes2025guidelines, sharkey2025mechanistic}. We use the term \emph{provenance and evidence alignment} to describe this traceability requirement, and \emph{research progression} to denote the structured continuation, evaluation, and validation of research problems across sessions and documents.

This paper synthesizes recent work spanning agentic architectures for scientific discovery, knowledge graph–based retrieval systems, empirical standards for software engineering research, methodological guidelines for LLM-based studies, and mechanistic interpretability. Rather than reviewing these contributions in isolation, we analyze them collectively to surface structural gaps in the current landscape. Building on this synthesis, we advance a provenance-first architectural framework for semi-autonomous research progression—one that embeds \emph{span-level evidence alignment} (the linking of structured knowledge graph nodes to specific sentences or clauses in source documents) into persistent, structured knowledge infrastructure so that generated outputs remain traceable and auditable across research sessions. The proposed system is designed to augment human research judgment, not replace it. While the architectural framework is intended to generalize, this dissertation focuses on software engineering as the primary domain of validation, leveraging the empirical standards and methodological guidelines already established in that community~\cite{ralph2021empirical, baltes2025guidelines}. Cross-domain applicability will be assessed through limited comparison with one additional field, but full general-purpose research automation is outside scope.

The remainder of this paper is organized as follows. Section~\ref{sec:literature} presents the literature assessment. Section~\ref{sec:proposal} develops the research proposal. Section~\ref{sec:conclusion} discusses implications and future directions.