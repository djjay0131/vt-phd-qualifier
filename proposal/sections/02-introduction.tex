\section{Introduction}
\label{sec:introduction}

Large language models have moved beyond isolated text generation into a new operational role: as components of \emph{semi-autonomous agentic systems} that perceive research contexts, invoke external tools, and coordinate multi-step workflows with limited human oversight~\cite{villaescusa2025denario, trinkenreich2025train}. Where earlier applications treated language models as question-answering interfaces, emerging architectures orchestrate multiple specialized agents—responsible for tasks such as literature retrieval, data analysis, and manuscript drafting—within pipelines capable of executing substantial portions of the research cycle~\cite{villaescusa2025denario}. This shift from isolated model usage to coordinated, tool-using agent systems represents a qualitative change in how AI participates in scientific work.

As these systems transition from experimental prototypes to components of real research workflows, the need for supporting infrastructure becomes increasingly evident. Architectural capability has advanced rapidly, yet the mechanisms for ensuring persistence, traceability, and evaluability have not matured at the same pace. Current approaches do not tightly integrate persistent knowledge representations with agentic reasoning, do not provide validated evaluation frameworks tailored to agent-generated outputs, and do not systematically enforce traceability between generated claims and their supporting evidence~\cite{suryawanshi2025kgrag, ralph2021empirical, baltes2025guidelines, sharkey2025mechanistic}. We use the term \emph{provenance and evidence alignment} to describe this traceability requirement, and \emph{research progression} to denote the structured continuation, evaluation, and validation of research problems across sessions and documents.

This paper synthesizes recent work spanning agentic architectures for scientific discovery, knowledge graph–based retrieval systems, empirical standards for software engineering research, methodological guidelines for LLM-based studies, and mechanistic interpretability. Rather than reviewing these contributions in isolation, we analyze them collectively to surface structural gaps in the current landscape. Building on this synthesis, we advance a provenance-first architectural framework for semi-autonomous research progression—one that embeds span-level evidence alignment into persistent, structured knowledge infrastructure so that generated outputs remain traceable and auditable across research sessions. The proposed system is designed to augment human research judgment, not replace it.

The remainder of this paper is organized as follows. Section~\ref{sec:literature} presents the literature assessment. Section~\ref{sec:proposal} develops the research proposal. Section~\ref{sec:conclusion} discusses implications and future directions.